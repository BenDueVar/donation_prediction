{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "051db67f",
   "metadata": {},
   "source": [
    "### Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975a3492",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from sklearn.feature_selection import f_classif, VarianceThreshold, SelectKBest, chi2\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7957c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('learningSet.csv') # original data\n",
    "df = pd.read_csv('learningSet.csv') # working data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a83f27",
   "metadata": {},
   "source": [
    "### Fill null values by criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c63971b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_missing_with_mean(df, column):\n",
    "    df[column].fillna(np.ceil(np.mean(df[column])), inplace=True)\n",
    "\n",
    "def fill_missing_with_median(df, column):\n",
    "    median_value = np.ceil(df[column].median(skipna=True))\n",
    "    df[column] = df[column].fillna(median_value)\n",
    "    \n",
    "def fill_missing_with_value(data, column, value):\n",
    "    data[column].fillna(value, inplace=True)\n",
    "\n",
    "# DOMAIN is the sum of the codes from DOMAIN A AND B. (A and B don't have nulls)\n",
    "df['DOMAIN'] = data['DOMAIN_A'] + data['DOMAIN_B'].astype(str)\n",
    "\n",
    "# empty SOLIH means that there is no limit for marketing solicitation from the customer\n",
    "fill_missing_with_value(df, 'SOLIH', 365)\n",
    "\n",
    "# 22% of income is null and is the leading category. I prefer to add a new label\n",
    "fill_missing_with_value(df, 'INCOME', 0)\n",
    "\n",
    "# Fill with the mode\n",
    "fill_missing_with_value(df, 'GENDER', 'F')\n",
    "\n",
    "# Fill missing values with the mean for 'DMA', 'CLUSTER2'\n",
    "for column in ['DMA', 'CLUSTER2']:\n",
    "    fill_missing_with_mean(df, column)\n",
    "\n",
    "# Fill 'TIMELAG' and 'NEXTDATE' with the ceiling of the median\n",
    "for column in ['TIMELAG', 'NEXTDATE']:\n",
    "    fill_missing_with_median(df, column)\n",
    "    \n",
    "# Fill cluster with the mode - 2.5% nulls\n",
    "fill_missing_with_value(df, 'CLUSTER', '40.0')\n",
    "\n",
    "# High irrelevancy - indicates with 22% nulls the source of the data\n",
    "fill_missing_with_value(df, 'DATASRCE', '0')\n",
    "\n",
    "# home owner flag - 23% nulls\n",
    "fill_missing_with_value(df, 'HOMEOWNR', 'U')\n",
    "\n",
    "def map_geocode(x):\n",
    "    if x not in ['A','B','C','D']:\n",
    "        return 'O'\n",
    "    else:\n",
    "        return x\n",
    "df['GEOCODE2'] = df['GEOCODE2'].apply(map_geocode)\n",
    "\n",
    "# For the columns that collect the reactions to magazine when null value is assumed that there is not reaction (no reaction=no information)\n",
    "columns_to_fill_zero = [\n",
    "    \"MBCRAFT\", \"MBGARDEN\", \"MBBOOKS\", \"MBCOLECT\", \"MAGFAML\", \"MAGFEM\", \"MAGMALE\",\n",
    "    \"PUBGARDN\", \"PUBCULIN\", \"PUBHLTH\", \"PUBDOITY\", \"PUBNEWFN\", \"PUBPHOTO\", \"PUBOPP\"\n",
    "]\n",
    "for column in columns_to_fill_zero:\n",
    "    df[column] = df[column].fillna(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73cfff1e",
   "metadata": {},
   "source": [
    "### Dropping by % of null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce276eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_null_percentages(df, threshold=0.33):\n",
    "    nulls_percent_df = pd.DataFrame(df.isna().sum() / len(df)).reset_index()\n",
    "    nulls_percent_df.columns = ['column_name', 'nulls_percentage']\n",
    "    return nulls_percent_df[nulls_percent_df['nulls_percentage'] > threshold]\n",
    "drop_columns_list = list(set(calculate_null_percentages(df)['column_name']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6a8a9b",
   "metadata": {},
   "source": [
    "### Dropping by relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8921d2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OSOURCE - symbol definitions not provided, too many categories \n",
    "# ZIP - we are including state already\n",
    "# MAILCODE is a flag for correctness of the address 98.5% are correct (94013/95412)\n",
    "# NOEXCH is another flag stating \"Do Not Exchange\" Flag (For list rental)\n",
    "# AGEFLAG states if the age is Exact or Inferred from Date of Birth Field\n",
    "# VETERANS (Y/N)\n",
    "irrelevant_list = ['OSOURCE', 'ZIP', 'MAILCODE', 'NOEXCH', 'AGEFLAG','VETERANS']\n",
    "# ADATE = Dates of past campaings\n",
    "adate_columns = [col for col in df.columns if \"ADATE_\" in col]\n",
    "\n",
    "drop_columns_list += irrelevant_list + adate_columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4fc8e3",
   "metadata": {},
   "source": [
    "### Dropping by redundancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d5de8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "redundant_list = ['WEALTH2','MSA', 'ADI','MAILCODE', 'DOMAIN_A','DOMAIN_B', 'MDMAUD']\n",
    "mdmaud_columns_rfa_columns = [col for col in data.columns if (\"RFA_\" in col or \"MDMAUD_\" in col)]\n",
    "\n",
    "# RFA_* (with RFA), MDMAUD_* (with MDMAUD). All dropped due to redundancy\n",
    "# Wealth 2 may be redundant in front of ICX Census data.\n",
    "# MSA, ADI, DMA are local market designations based on marketing communication. ADI and DMA give similar segmentations (but ranked differently), MSA takes only county clusters above 50,000 inhabitants.\n",
    "# Domain A and B as they are integrated in DOMAIN\n",
    "# MDMAUD is in MAJOR\n",
    "\n",
    "drop_columns_list += redundant_list + mdmaud_columns_rfa_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936e42f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=drop_columns_list, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59b39f7",
   "metadata": {},
   "source": [
    "### Condensing groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375e5049",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_gender(x):\n",
    "    return x if x in ['F', 'M'] else 'other'\n",
    "df['GENDER'] = df['GENDER'].apply(map_gender)\n",
    "\n",
    "    \n",
    "state_count = df['STATE'].value_counts().reset_index()\n",
    "state_count.columns = ['state', 'count']\n",
    "other_states = state_count[state_count['count'] < 2500]['state'].tolist()\n",
    "\n",
    "def map_state(x):\n",
    "    return 'other' if x in other_states else x\n",
    "df['STATE'] = df['STATE'].apply(map_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf643d1",
   "metadata": {},
   "source": [
    "### Mapping for rebuild categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea128cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['1', '2', '3', '4', '5']\n",
    "age_group = df['AGE']\n",
    "age_group = pd.cut(age_group,bins=5,labels=names, include_lowest=True)\n",
    "default = 6\n",
    "age_group= age_group.cat.add_categories([default])\n",
    "age_group = age_group.fillna(default)\n",
    "df['AGE'] = age_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34bd3d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count nulls, reset index, and filter columns with nulls\n",
    "null_count = df.isnull().sum().reset_index().rename(columns={'index': 'column', 0: 'nulls'})\n",
    "null_count = null_count[null_count['nulls'] > 0]\n",
    "null_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370edfba",
   "metadata": {},
   "source": [
    "### Select data by type: \n",
    "### Separate into numerical and categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3666ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = df[['TARGET_B', 'TARGET_D']]\n",
    "numerical = df.select_dtypes('number').drop(columns = ['TARGET_B', 'TARGET_D'])\n",
    "categorical = df.select_dtypes('object')\n",
    "# In this summary the division by data type is done in a second phase\n",
    "print(numerical.shape)\n",
    "print(categorical.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912e9c12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3c4efde4",
   "metadata": {},
   "source": [
    "### Space values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d71627",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'df' is your DataFrame\n",
    "string_to_count = ' '\n",
    "\n",
    "# Define a function that safely applies the count method to strings\n",
    "def safe_count(x):\n",
    "    if isinstance(x, str):\n",
    "        return x.count(string_to_count)\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# Apply the safe_count function to each element of the DataFrame\n",
    "counts = df.applymap(safe_count).sum()\n",
    "\n",
    "# Filter the counts to only show columns with a count greater than 0\n",
    "counts = counts[counts > 0]\n",
    "\n",
    "# This will give you a Series with the count of the string in each column where the count is greater than 0\n",
    "print(counts)\n",
    "\n",
    "# According to further documentation \n",
    "\n",
    "# Blank PEPSTRFL is no PEP Star\n",
    "# Are binary categories eligible for a negative statement sustitution PEPSTRFL, RECINHSE, RECP3, RECPGVG, RECSWEEP, PVASTATE, MAJOR (pd.concat([data['MDMAUD'], df['MAJOR']],axis=1).value_counts())^1, COLLECT1, BIBLE, CATLG, HOMEE, PETS,CDPLAY, STEREO,PCOWNERS,PHOTO,CRAFTS,FISHER,GARDENIN,BOATS,WALKER,KIDSTUFF,CARDS,PLATES\n",
    "# Non binary categories but still eligible for negative statement sustitution CHILD* (with N), SOLP3 (change with 365), HOMEOWNR but the negative label is already defined 'U',\n",
    "# ^1 there is an error in MAJOR column description as X is indicating major donors where the document says the opposite, but after comparing with MDMAUD is clear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18db0d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace space values with \"365\" in the \"SOLP3\" column\n",
    "categorical['SOLP3'] = categorical['SOLP3'].replace(' ', '365')\n",
    "# Replace space values with \"U\" in the \"HOMEOWNR\" column\n",
    "categorical['HOMEOWNR'] = categorical['HOMEOWNR'].replace(' ', 'U')\n",
    "\n",
    "def replace_space_with_N(columns, dataframe):\n",
    "    for column in columns:\n",
    "        dataframe[column] = dataframe[column].apply(lambda x: 'N' if x == ' ' else x)\n",
    "    return dataframe\n",
    "\n",
    "# List of columns to apply the replacement\n",
    "columns_to_replace = [\n",
    "    'PEPSTRFL', 'RECINHSE', 'RECP3', 'RECPGVG', 'RECSWEEP', 'PVASTATE', 'MAJOR',\n",
    "    'COLLECT1', 'BIBLE', 'CATLG', 'HOMEE', 'PETS', 'CDPLAY', 'STEREO', 'PCOWNERS',\n",
    "    'PHOTO', 'CRAFTS', 'FISHER', 'GARDENIN', 'BOATS', 'WALKER', 'KIDSTUFF', 'CARDS', 'PLATES'\n",
    "]\n",
    "\n",
    "# Apply the function to the 'categorical' DataFrame\n",
    "categorical = replace_space_with_N(columns_to_replace, categorical)\n",
    "\n",
    "# Replace space values with \"N\" in columns that start with \"CHILD\"\n",
    "child_columns = [col for col in categorical.columns if col.startswith('CHILD')]\n",
    "categorical = replace_space_with_N(child_columns, categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f56482",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_2 = categorical.applymap(safe_count).sum()\n",
    "\n",
    "# Filter the counts to only show columns with a count greater than 0\n",
    "counts = counts_2[counts_2 > 0]\n",
    "\n",
    "# This will give you a Series with the count of the string in each column where the count is greater than 0\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ff79c7",
   "metadata": {},
   "source": [
    "### Check distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242db402",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "for column in numerical.columns:\n",
    "    fig = px.histogram(numerical, x=column)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34181bdd",
   "metadata": {},
   "source": [
    "### Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae05e42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine features\n",
    "X = pd.concat([numerical, categorical], axis=1)\n",
    "y = df['TARGET_B']\n",
    "\n",
    "X2 = X\n",
    "y2 = df['TARGET_D']\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d36e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_cat = X_train.select_dtypes(include = object).astype(str)\n",
    "X_train_num =X_train.select_dtypes(include = np.number)\n",
    "X_test_cat = X_test.select_dtypes(include = object).astype(str)\n",
    "X_test_num =X_test.select_dtypes(include = np.number)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228cd622",
   "metadata": {},
   "source": [
    "### Encode and scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ebcb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categorical features\n",
    "encoder = OneHotEncoder(drop='first').fit(X_train_cat.select_dtypes(include=object))\n",
    "\n",
    "cols = encoder.get_feature_names_out(input_features=X_train_cat.columns)\n",
    "X_train_cat_encode = pd.DataFrame(encoder.transform(X_train_cat).toarray(),columns=cols)\n",
    "X_train_cat_encode.reset_index(drop = True, inplace = True)\n",
    "\n",
    "cols = encoder.get_feature_names_out(input_features=X_test_cat.columns)\n",
    "X_test_cat_encode = pd.DataFrame(encoder.transform(X_test_cat).toarray(),columns=cols)\n",
    "X_test_cat_encode.reset_index(drop = True, inplace = True)\n",
    "\n",
    "# Scale numerical features\n",
    "transformer = MinMaxScaler().fit(X_train_num)\n",
    "\n",
    "X_train_num_norm = transformer.transform(X_train_num)\n",
    "X_train_num_scale = pd.DataFrame(X_train_num_norm, index = X_train_num.index, columns=X_train_num.columns)\n",
    "X_train_num_scale.reset_index(drop = True, inplace = True)\n",
    "\n",
    "X_test_num_norm = transformer.transform(X_test_num)\n",
    "X_test_num_scale = pd.DataFrame(X_test_num_norm, index = X_test_num.index, columns=X_test_num.columns)\n",
    "X_test_num_scale.reset_index(drop = True, inplace=True)\n",
    "\n",
    "\n",
    "# Concatenate X train and test and resume y train and y test\n",
    "X_train_processed = pd.concat([X_train_num_scale,X_train_cat_encode], axis=1)\n",
    "y_train.reset_index(drop = True, inplace = True)\n",
    "\n",
    "X_test_processed = pd.concat([X_test_num_scale,X_test_cat_encode], axis=1)\n",
    "y_test.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a77de4",
   "metadata": {},
   "source": [
    "### Feature selection (Variance Threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7446be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature selection based on a variance threshold. Does not consider the relationship between features and the target variable.\n",
    "\n",
    "var_threshold = 0.13\n",
    "selector_var = VarianceThreshold(threshold=var_threshold)\n",
    "X_train_var = selector_var.fit_transform(X_train_processed)\n",
    "X_test_var = selector_var.transform(X_test_processed)\n",
    "\n",
    "indices_var = selector_var.get_support(indices=True)\n",
    "len(indices_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6977f3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# selector_var.variances_ > var_threshold\n",
    "# drop_list = [col[0] for col in zip(X_train.columns,var_list) if col[1] == False]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7595abb1",
   "metadata": {},
   "source": [
    "### Feature selection (SelectKBest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3cda4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features that have the strongest relationship with the target variable, based on the chi-squared statistic. Only positive values, and the best for categorical. \n",
    "\n",
    "selector_chi2 = SelectKBest(chi2, k=20)\n",
    "X_train_chi2 = selector_chi2.fit_transform(X_train_processed, y_train)\n",
    "X_test_chi2 = selector_chi2.transform(X_test_processed)\n",
    "\n",
    "indices_chi2 = selector_chi2.get_support(indices=True)\n",
    "len(indices_chi2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc5b667",
   "metadata": {},
   "source": [
    "### Feature selection (ANOVA F-test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2a62d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selection based on univariate statistical tests. May not capture non-linear relationships as effectively. Good for numerical and categorical\n",
    "\n",
    "selector_f = SelectKBest(f_classif, k=20)\n",
    "X_train_f_test = selector_f.fit_transform(X_train_processed, y_train)\n",
    "X_test_f_test = selector_f.transform(X_test_processed)\n",
    "\n",
    "indices_f = selector_f.get_support(indices=True)\n",
    "len(indices_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3408c0f",
   "metadata": {},
   "source": [
    "### Feature selection (Random Forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a3bfe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An ensemble method that uses feature importance scores from the Random Forest classifier to select features. Computational intensive. Good for numerical and categorical.Can capture both linear and non-linear relationships; provides feature importance scores.\n",
    "\n",
    "forest = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "forest.fit(X_train_processed, y_train)\n",
    "importances = forest.feature_importances_\n",
    "\n",
    "\n",
    "indices_rf = np.argsort(importances)[::-1][:20]  # Top 20 features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b021e2c",
   "metadata": {},
   "source": [
    "### Feature comparison table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7a88de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the maximum length of the lists\n",
    "max_length = max(len(indices_var), len(indices_chi2), len(indices_f), len(indices_rf))\n",
    "\n",
    "# Function to pad lists to the maximum length\n",
    "def pad_list(lst, length, pad_value):\n",
    "    return lst + [pad_value] * (length - len(lst))\n",
    "\n",
    "# Pad the lists\n",
    "indices_var_padded = pad_list(list(X_train_processed.columns[indices_var]), max_length, None)\n",
    "indices_chi2_padded = pad_list(list(X_train_processed.columns[indices_chi2]), max_length, None)\n",
    "indices_f_padded = pad_list(list(X_train_processed.columns[indices_f]), max_length, None)\n",
    "indices_rf_padded = pad_list(list(X_train_processed.columns[indices_rf]), max_length, None)\n",
    "\n",
    "# Sort each column individually\n",
    "sorted_var = sorted([x for x in indices_var_padded if x is not None])\n",
    "sorted_chi2 = sorted([x for x in indices_chi2_padded if x is not None])\n",
    "sorted_f = sorted([x for x in indices_f_padded if x is not None])\n",
    "sorted_rf = sorted([x for x in indices_rf_padded if x is not None])\n",
    "\n",
    "# Pad the sorted lists to the maximum length\n",
    "sorted_var_padded = pad_list(sorted_var, max_length, None)\n",
    "sorted_chi2_padded = pad_list(sorted_chi2, max_length, None)\n",
    "sorted_f_padded = pad_list(sorted_f, max_length, None)\n",
    "sorted_rf_padded = pad_list(sorted_rf, max_length, None)\n",
    "\n",
    "# Now create the DataFrame with sorted columns\n",
    "sorted_comparison_df = pd.DataFrame({\n",
    "    'Variance Threshold': sorted_var_padded,\n",
    "    'SelectKBest chi2': sorted_chi2_padded,\n",
    "    'ANOVA F-test': sorted_f_padded,\n",
    "    'Random Forest': sorted_rf_padded\n",
    "})\n",
    "\n",
    "print(sorted_comparison_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd9bffc",
   "metadata": {},
   "source": [
    "### Looking at multicolinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb1bc07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the X_train datasets based on selected features\n",
    "X_train_var = X_train_processed.iloc[:, indices_var]\n",
    "X_train_rf = X_train_processed.iloc[:, indices_rf]\n",
    "X_train_f = X_train_processed.iloc[:, indices_f]\n",
    "X_train_chi2 = X_train_processed.iloc[:, indices_chi2]\n",
    "\n",
    "X_test_var = X_test_processed.iloc[:, indices_var]\n",
    "X_test_rf = X_test_processed.iloc[:, indices_rf]\n",
    "X_test_f = X_test_processed.iloc[:, indices_f]\n",
    "X_test_chi2 = X_test_processed.iloc[:, indices_chi2]\n",
    "\n",
    "# Define a function to compute and plot the correlation matrix\n",
    "def plot_correlation_matrix(X, title):\n",
    "    corr_matrix = pd.concat([X,y_train], axis=1).corr(method='pearson')\n",
    "    plt.figure(figsize=(20, 15))\n",
    "    sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm')\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "# Plot the correlation matrix for each X_train dataset\n",
    "plot_correlation_matrix(X_train_var, 'Variance Threshold Feature Correlation Matrix')\n",
    "plot_correlation_matrix(X_train_rf, 'Random Forest Feature Correlation Matrix')\n",
    "plot_correlation_matrix(X_train_f, 'ANOVA F-test Feature Correlation Matrix')\n",
    "plot_correlation_matrix(X_train_chi2, 'SelectKBest chi2 Feature Correlation Matrix')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9190d9b0",
   "metadata": {},
   "source": [
    "### Drop highest multi-collinearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1d99ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# High correlation between features in the following cases:\n",
    "# X_train_var: POP90C1, HVP4\n",
    "# X_train_rf: MINRDATE, FISTDATE, CARDPROM, POP903, POP902, NGIFTALL\n",
    "# X_train_f: HV2, HVP4, CARDPROM, FISTDATE,FISTDATE_YR \n",
    "# X_train_chi2: HV2, HV3, HV4, HV6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779b4a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_columns(df, columns_to_drop):\n",
    "    df = df.drop(columns_to_drop, axis=1, errors='ignore')\n",
    "    return df\n",
    "\n",
    "# Drop the specified columns from X_train_var and X_test_var\n",
    "X_train_var = drop_columns(X_train_var, ['POP90C1', 'HVP4'])\n",
    "X_test_var = drop_columns(X_test_var, ['POP90C1', 'HVP4'])\n",
    "\n",
    "# Drop the specified columns from X_train_rf and X_test_rf\n",
    "X_train_rf = drop_columns(X_train_rf, ['MINRDATE', 'FISTDATE', 'CARDPROM', 'POP903', 'POP902', 'NGIFTALL'])\n",
    "X_test_rf = drop_columns(X_test_rf, ['MINRDATE', 'FISTDATE', 'CARDPROM', 'POP903', 'POP902', 'NGIFTALL'])\n",
    "\n",
    "# Drop the specified columns from X_train_f and X_test_f\n",
    "X_train_f = drop_columns(X_train_f, ['HV2', 'HVP4', 'CARDPROM', 'FISTDATE', 'FISTDATE_YR'])\n",
    "X_test_f = drop_columns(X_test_f, ['HV2', 'HVP4', 'CARDPROM', 'FISTDATE', 'FISTDATE_YR'])\n",
    "\n",
    "# Drop the specified columns from X_train_chi2 and X_test_chi2\n",
    "X_train_chi2 = drop_columns(X_train_chi2, ['HV2', 'HV3', 'HV4', 'HV6'])\n",
    "X_test_chi2 = drop_columns(X_test_chi2, ['HV2', 'HV3', 'HV4', 'HV6'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898afa86",
   "metadata": {},
   "source": [
    "## Pre-Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1f73f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "%time\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "def evaluate_model(X_train, X_test, y_train, y_test, model_name):\n",
    "    clf = RandomForestClassifier(\n",
    "    n_estimators=100,      # More trees can lead to better performance but also to longer training times\n",
    "    max_depth=20,          # Deeper trees capturing more complex patterns. Be cautious, as too much depth can lead to overfitting\n",
    "    min_samples_split=2,   # Minimum number of samples required to split an internal node. Lower values allow the algorithm to create more specific splits, thus increasing complexity.\n",
    "    min_samples_leaf=1,    # Minimum number of samples required to be at a leaf node. Smaller leaf sizes result in more complex decision paths\n",
    "    )\n",
    "    clf.fit(X_train, y_train)\n",
    "    train_accuracy = clf.score(X_train, y_train)\n",
    "    test_accuracy = clf.score(X_test, y_test)\n",
    "    print(f'{model_name} - TrainSet Accuracy: ', train_accuracy)\n",
    "    print(f'{model_name} - TestSet Accuracy: ', test_accuracy)\n",
    "    \n",
    "    # Compute confusion matrix\n",
    "    y_pred = clf.predict(X_test)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    # Plotting the confusion matrix\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "    disp.plot(cmap=plt.cm.Blues)\n",
    "    plt.title(f'Confusion Matrix for {model_name}')\n",
    "    plt.show()\n",
    "\n",
    "    return train_accuracy, test_accuracy\n",
    "\n",
    "# Example usage:\n",
    "evaluate_model(X_train_var, X_test_var, y_train, y_test, 'X_train_var')\n",
    "evaluate_model(X_train_rf, X_test_rf, y_train, y_test, 'X_train_rf')\n",
    "evaluate_model(X_train_f, X_test_f, y_train, y_test, 'X_train_f')\n",
    "evaluate_model(X_train_chi2, X_test_chi2, y_train, y_test, 'X_train_chi2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a5d692",
   "metadata": {},
   "source": [
    "### Check and fix Target imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4991375",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train.value_counts())\n",
    "target_d_1 = len(y2_train[y2_train>0])\n",
    "target_d_0 = len(y2_train)\n",
    "print('TARGET_D\\n', target_d_0, '\\n', target_d_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27c4201",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "df = pd.concat([X_train_processed,X_test_processed], axis=1)\n",
    "df_b = pd.concat([df,y],axis=1)\n",
    "df_d = pd.concat([df,y2],axis=1)\n",
    "\n",
    "# Separate the majority and minority classes\n",
    "df_b_majority = df_b[df_b.TARGET_B == 0]\n",
    "df_b_minority = df_b[df_b.TARGET_B == 1]\n",
    "\n",
    "df_d_majority = df_d[df_d.TARGET_D == 0]\n",
    "df_d_minority = df_d[df_d.TARGET_D > 0]\n",
    "\n",
    "\n",
    "# Upsample minority class\n",
    "df_b_minority_upsampled = resample(df_b_minority, \n",
    "                                 replace=True,\n",
    "                                 n_samples=len(df_b_majority),\n",
    "                                 random_state=101)\n",
    "\n",
    "df_d_minority_upsampled = resample(df_d_minority, \n",
    "                                 replace=True,\n",
    "                                 n_samples=len(df_d_majority),\n",
    "                                 random_state=101)\n",
    "\n",
    "\n",
    "# Combine majority class with upsampled minority class\n",
    "df_b_upsampled = pd.concat([df_b_majority, df_b_minority_upsampled])\n",
    "df_d_upsampled = pd.concat([df_d_majority, df_d_minority_upsampled])\n",
    "\n",
    "# Display new class counts\n",
    "print(df_b_upsampled.TARGET_B.value_counts())\n",
    "print(df_d_upsampled.TARGET_D.value_counts())\n",
    "\n",
    "# X and Y for TARGET B\n",
    "X_upsampled = df_b_upsampled.drop('TARGET_B', axis=1)\n",
    "y_upsampled = df_b_upsampled.TARGET_B\n",
    "\n",
    "# X and Y for TARGET B\n",
    "X2_upsampled = df_d_upsampled.drop('TARGET_D', axis=1)\n",
    "y2_upsampled = df_d_upsampled.TARGET_D\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29321fa5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f4286dcb",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a449af79",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(max_depth=5,\n",
    "                             min_samples_split=20,\n",
    "                             min_samples_leaf =20)\n",
    "clf.fit(X_train_oversampled, y_train_oversampled)\n",
    "print('TrainSet = ',clf.score(X_train_oversampled, y_train_oversampled))\n",
    "print('TestSet = ',clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2743f2eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ba2ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pred_RF = clf2.predict(X_test)\n",
    "\n",
    "print('accuracy:', accuracy_score(y_test, pred_RF))\n",
    "print(\"precision: \",precision_score(y_test,pred_RF))\n",
    "print(\"recall: \",recall_score(y_test,pred_RF))\n",
    "print(\"f1: \",f1_score(y_test,pred_RF))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c58128",
   "metadata": {},
   "source": [
    "### Check the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eaa9c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(clf2, X_test, y_test,cmap=plt.cm.Blues)  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e29db30",
   "metadata": {},
   "source": [
    "# Now we have a model, let's run ALL of the data to gain insights for the business"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c635049",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635d7f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(X)\n",
    "display(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6fc55c",
   "metadata": {},
   "source": [
    "## Scale and encode ALL of the data for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b94288",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xcat = X.select_dtypes(include=object)\n",
    "Xnum = X.select_dtypes(include=np.number)\n",
    "Xcat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a324ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoder = OneHotEncoder(drop='first').fit(Xcat)\n",
    "\n",
    "cols = encoder.get_feature_names(input_features=Xcat.columns)\n",
    "\n",
    "Xcat_encode = pd.DataFrame(encoder.transform(Xcat).toarray(),columns=cols)\n",
    "\n",
    "Xcat_encode.reset_index(drop = True, inplace = True)\n",
    "Xcat_encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d1c33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#transformer = MinMaxScaler().fit(Xnum)\n",
    "Xnum_norm = transformer.transform(Xnum)\n",
    "print(Xnum_norm.shape)\n",
    "Xnum_scale = pd.DataFrame(Xnum_norm, index = Xnum.index, columns=Xnum.columns)\n",
    "Xnum_scale.head()\n",
    "Xnum_scale.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648b7f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tarb = pd.concat([Xnum_scale,Xcat_encode], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffbe766",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tarb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9474835a",
   "metadata": {},
   "source": [
    "# Keep same columns as before (top 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e28b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "X2 = X_tarb[col_list]           #columns to keep -- top 25\n",
    "X2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385650af",
   "metadata": {},
   "outputs": [],
   "source": [
    "X2 = X2.drop(col_to_drop, axis = 1)\n",
    "X2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d4a1b7",
   "metadata": {},
   "source": [
    "## Re-running Classifier with all the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24667a9d",
   "metadata": {},
   "source": [
    "### Run the classifier on all of the data, then look at the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbacbabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Using the upsampled classifier\n",
    "\n",
    "print('All data = ',clf2.score(X2,y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9208bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(clf2, X2,y,cmap=plt.cm.Blues)  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ffa4851",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_RF = clf2.predict(X2)\n",
    "\n",
    "print('accuracy:', accuracy_score(y, pred_RF))\n",
    "print(\"precision: \",precision_score(y,pred_RF))\n",
    "print(\"recall: \",recall_score(y,pred_RF))\n",
    "print(\"f1: \",f1_score(y,pred_RF))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632f4c88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8c7748",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8c08d242",
   "metadata": {},
   "source": [
    "### All data with target B predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8143deb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pred_tarB = X\n",
    "all_pred_tarB['predicted_B']= pred_RF\n",
    "all_pred_tarB['target_D'] = target_d\n",
    "all_pred_tarB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0189c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d9a06720",
   "metadata": {},
   "source": [
    "## Finding mean of Target D to understand data Target B results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3cd516",
   "metadata": {},
   "source": [
    "#### We will use data_targetD for part 2 (regression) of the process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c5396c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_targetD = full_data[full_data['TARGET_B']==1]\n",
    "data_targetD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d4f04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_targetD['TARGET_D'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee70a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_targetD['TARGET_D'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38eb394",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_targetD['TARGET_D'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3df673f",
   "metadata": {},
   "source": [
    "# Reading the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eada936d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(clf2, X2,y,cmap=plt.cm.Blues)  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8cf82bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_y = clf2.predict(X2)\n",
    "\n",
    "\n",
    "array = confusion_matrix(y, predict_y)\n",
    "Q1 = array[0][0]\n",
    "Q2 = array[0][1]\n",
    "Q3 = array[1][0]\n",
    "Q4 = array[1][1]\n",
    "print(Q1)\n",
    "print(Q2)\n",
    "print(Q3)\n",
    "print(Q4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd96e4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "donation_gained = Q4 * 15.6\n",
    "unexpected_donation = Q3 * 15.6\n",
    "spend_marketing = (Q4+Q2) * 0.68\n",
    "no_return_marketing = Q2 * 0.68\n",
    "overall_revenue = (donation_gained + unexpected_donation) - spend_marketing\n",
    "\n",
    "\n",
    "\n",
    "print('Donation amount gained:',round(donation_gained,2))\n",
    "print('Unexpected Donations:',round(unexpected_donation,2))\n",
    "print('Amount spent (cost) on marketing:',round(spend_marketing,2))\n",
    "print('Amount lost on marketing:',round(no_return_marketing,2))\n",
    "\n",
    "print('Overall revenue (donation - cost):',round(overall_revenue,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2c1ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('In the above plot we can see that there are    ',Q3,'    people of whom the model predicts they will not donate,')\n",
    "print('when they actually would donate, AKA falsely predicted non-donators.')\n",
    "print('We gain a donation amount of    ',unexpected_donation,'    from this group.')\n",
    "print('There are also   ', Q2 ,'    people of whom the model predicts they will donate, when they actually do not.')\n",
    "print('We  lose   ', no_return_marketing,'   on mailing costs from this group.')\n",
    "print('When we look at the average donation amount of someone who actually donates is around 15.62 dollars.')\n",
    "print('We raise   ', overall_revenue ,'   dollars with this model.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6413a7",
   "metadata": {},
   "source": [
    "## Results of the model \n",
    "\n",
    "\n",
    "Zooming in on the costs to send the marketing packages: in this scenario we spend a total of 26158 dollars on marketing. Of which 24208 dollars does not yield any donation. It is suggested that mailers be sent to our predicted donors as well as people who have donated in the past.\n",
    "\n",
    "We can clearly conclude that it's in the best interest of Healthcare 4 All to have a model in which we have the lowest amount of falsely predicted non-donators. With this model we can see that the donation amount lost is almost equal to the Unexpected Donations gained.This model would need to be improved to optimize for the Falsely Predicted non- donators who would actually donate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3198b0c7",
   "metadata": {},
   "source": [
    "# Moving on to create model to predict HOW MUCH they will donate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b10395",
   "metadata": {},
   "source": [
    "# Predicting amount given among actual donators"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307089c2",
   "metadata": {},
   "source": [
    "#### Using target D dataset created earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3552a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_targetD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb11bfe",
   "metadata": {},
   "source": [
    "#### Breaking into features and target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e9a011",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_D = data_targetD.drop(['TARGET_D', 'TARGET_B', 'Unnamed: 0'], axis=1)\n",
    "y_D = data_targetD['TARGET_D']\n",
    "display(X_D)\n",
    "display(y_D)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a8f1ad",
   "metadata": {},
   "source": [
    "#### Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d743b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(X_D, y_D, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c93b9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_cat1 = X_train1.select_dtypes(include = object)\n",
    "X_train_num1 =X_train1.select_dtypes(include = np.number)\n",
    "X_test_cat1 = X_test1.select_dtypes(include = object)\n",
    "X_test_num1 =X_test1.select_dtypes(include = np.number)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aab088b",
   "metadata": {},
   "source": [
    "## Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18a62bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = OneHotEncoder(drop='first').fit(X_train_cat1)\n",
    "\n",
    "cols1 = encoder.get_feature_names(input_features=X_train_cat1.columns)\n",
    "\n",
    "X_train_cat_encode1 = pd.DataFrame(encoder.transform(X_train_cat1).toarray(),columns=cols1)\n",
    "\n",
    "X_train_cat_encode1.reset_index(drop = True, inplace = True)\n",
    "X_train_cat_encode1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86b856a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = encoder.get_feature_names(input_features=X_test_cat1.columns)\n",
    "\n",
    "X_test_cat_encode1 = pd.DataFrame(encoder.transform(X_test_cat1).toarray(),columns=cols)\n",
    "\n",
    "X_test_cat_encode1.reset_index(drop = True, inplace = True)\n",
    "X_test_cat_encode1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57a1d4e",
   "metadata": {},
   "source": [
    "## Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688fda14",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = MinMaxScaler().fit(X_train_num1)\n",
    "X_train_num_norm1 = transformer.transform(X_train_num1)\n",
    "print(X_train_num_norm1.shape)\n",
    "X_train_num_scale1 = pd.DataFrame(X_train_num_norm1, index = X_train_num1.index, columns=X_train_num1.columns)\n",
    "X_train_num_scale1.head()\n",
    "X_train_num_scale1.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59659388",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_num_norm1 = transformer.transform(X_test_num1)\n",
    "print(X_test_num_norm1.shape)\n",
    "X_test_num_scale1 = pd.DataFrame(X_test_num_norm1, index = X_test_num1.index, columns=X_test_num1.columns)\n",
    "X_test_num_scale1.head()\n",
    "X_test_num_scale1.reset_index(drop = True, inplace=True)\n",
    "X_test_num_scale1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6a9c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train1 = pd.concat([X_train_num_scale1,X_train_cat_encode1], axis=1)\n",
    "X_train1.index\n",
    "y_train1.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749e5e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test1 = pd.concat([X_test_num_scale1,X_test_cat_encode1], axis=1)\n",
    "X_test1\n",
    "y_test1.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af95b46c",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1163f9c",
   "metadata": {},
   "source": [
    "#### Variance - Not Used Code included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f597787a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_selection import VarianceThreshold\n",
    "# Var_threshold = 0.02\n",
    "# sel = VarianceThreshold(threshold=Var_threshold)\n",
    "                        \n",
    "# sel = sel.fit(X_train1)\n",
    "# temp = sel.transform(X_train1)\n",
    "# temp = pd.DataFrame(temp)\n",
    "# print(X_train1.shape)\n",
    "# print(temp.shape)\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1070558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sel.variances_ > Var_threshold\n",
    "# sel.get_support()\n",
    "# var_list = list(sel.get_support())\n",
    "# var_list\n",
    "# print(var_list.count(True))\n",
    "# print(var_list.count(False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a814b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# zipped = list(zip(X_train.columns,var_list))\n",
    "# len(zipped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79ebe09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop_list = [col[0] for col in zip(X_train1.columns,var_list) if col[1] == False]\n",
    "# print(drop_list)\n",
    "# drop_list = drop_list +['HVP1','HVP2','HVP3','HVP5','HVP6','HV2']\n",
    "# len(drop_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ed5959",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4165df5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train1 = X_train1.drop(drop_list, axis = 1)\n",
    "# X_train1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938f9491",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test1 = X_test1.drop(drop_list, axis = 1)\n",
    "# X_test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4e6e5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bfd7c042",
   "metadata": {},
   "source": [
    "# PCA - Principle Component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1165385b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27147556",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(0.9)\n",
    "pca.fit(X_train1)\n",
    "\n",
    "X_train_pca = pca.transform(X_train1)\n",
    "X_test_pca = pca.transform(X_test1)\n",
    "\n",
    "corr_pc = pd.DataFrame(X_train_pca).corr()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9036d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_var_pca = pca.explained_variance_ratio_\n",
    "cum_sum_eigenvalues = np.cumsum(exp_var_pca)\n",
    "\n",
    "\n",
    "# Create the visualization plot\n",
    "plt.bar(range(0,len(exp_var_pca)), exp_var_pca, \n",
    "        alpha=0.5, align='center', label='Individual explained variance')\n",
    "plt.step(range(0,len(cum_sum_eigenvalues)), cum_sum_eigenvalues, \n",
    "         where='mid',label='Cumulative explained variance')\n",
    "plt.ylabel('Explained variance ratio')\n",
    "plt.xlabel('Principal component index')\n",
    "plt.legend(loc='best')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b74bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train_pca.shape)\n",
    "print(X_test_pca.shape)\n",
    "print (y_train1.shape)\n",
    "print(y_test1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98052adb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "79549b98",
   "metadata": {},
   "source": [
    "## RFE option, not used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881a94bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_selection import RFE\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# # define the method\n",
    "# rfe = RFE(estimator=DecisionTreeClassifier(), n_features_to_select=30)\n",
    "# # fit the model\n",
    "# rfe.fit(X_train, y_train)\n",
    "# # transform the data\n",
    "# X, y = rfe.transform(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cabd5c5e",
   "metadata": {},
   "source": [
    "# Run regression models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4e49a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a75d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def models_automation(models, X_train1, y_train1,X_test1,y_test1):\n",
    "    for model in models:\n",
    "        model.fit(X_train1, y_train1)\n",
    "        print(f\"{model.__class__.__name__}: Train -> {model.score(X_train1, y_train1)}, Test -> {model.score(X_test1, y_test1)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8333e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list = [LinearRegression(),SGDRegressor(),KNeighborsRegressor(), MLPRegressor(),DecisionTreeRegressor(),RandomForestRegressor()]\n",
    "models_automation(model_list, X_train_pca, y_train1,X_test_pca,y_test1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dee0398",
   "metadata": {},
   "source": [
    "### Optimize parameters for models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b93644",
   "metadata": {},
   "source": [
    "### Opted to use the non-PCA train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a6e1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list = [LinearRegression(),SGDRegressor(),KNeighborsRegressor(n_neighbors =9), MLPRegressor(solver = 'lbfgs', max_iter = 1500),DecisionTreeRegressor(criterion=\"poisson\"),\n",
    "              RandomForestRegressor(min_samples_split = 7,\n",
    "                                    min_samples_leaf = 6,\n",
    "                                    max_samples = 0.4,\n",
    "                                    max_depth = 14,\n",
    "                                    n_jobs = -1)]\n",
    "models_automation(model_list, X_train1, y_train1,X_test1,y_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c53f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression / Random Forest ??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4286a0",
   "metadata": {},
   "source": [
    "# Predict how much our predicted Donors will give."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0bb3a2",
   "metadata": {},
   "source": [
    "#### Create dataframe of ONLY predicted donors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eecf2b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_donors = all_pred_tarB[all_pred_tarB['predicted_B']==1]\n",
    "pred_donors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81b60e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_B_X=pred_donors.drop(['predicted_B','target_D'], axis=1)\n",
    "pred_B_y=pred_donors['target_D']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b424110",
   "metadata": {},
   "source": [
    "#### Split for scaling and encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94500e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_B_X_cat = pred_B_X.select_dtypes(include = object)\n",
    "pred_B_X_num =pred_B_X.select_dtypes(include = np.number)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1aefd89",
   "metadata": {},
   "source": [
    "#### Scale - using existing scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b795ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_B_X_num_norm1 = transformer.transform(pred_B_X_num)\n",
    "print(pred_B_X_num_norm1.shape)\n",
    "pred_B_X_num_scale1 = pd.DataFrame(pred_B_X_num_norm1, index = pred_B_X_num.index, columns=pred_B_X_num.columns)\n",
    "pred_B_X_num_scale1.head()\n",
    "pred_B_X_num_scale1.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf8b304",
   "metadata": {},
   "source": [
    "#### Encode - using existing encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642b1600",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = encoder.get_feature_names(input_features=pred_B_X_cat.columns)\n",
    "\n",
    "pred_B_X_cat_encode1 = pd.DataFrame(encoder.transform(pred_B_X_cat).toarray(),columns=cols)\n",
    "\n",
    "pred_B_X_cat_encode1.reset_index(drop = True, inplace = True)\n",
    "pred_B_X_cat_encode1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d40c43",
   "metadata": {},
   "source": [
    "#### Concatenate together for all Predicted X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250bc153",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_B_X = pd.concat([pred_B_X_num_scale1,pred_B_X_cat_encode1], axis=1)\n",
    "pred_B_X.index\n",
    "pred_B_y.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6c6d60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "93fc1391",
   "metadata": {},
   "source": [
    "## Run same PCA Transformer - Not used but here to try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96824b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_B_X_pca = pca.transform(pred_B_X)\n",
    "# pred_B_X_pca.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2746a31d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4bcd5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e7a71324",
   "metadata": {},
   "source": [
    "### Run through model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b243067",
   "metadata": {},
   "source": [
    "#### First a Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc5d8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3bef2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfr= RandomForestRegressor(min_samples_split = 7,\n",
    "                                    min_samples_leaf = 6,\n",
    "                                    max_samples = 0.4,\n",
    "                                    max_depth = 14,\n",
    "                                    n_jobs = -1).fit(X_train1, y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feda0699",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfr_pred = rfr.predict(pred_B_X)\n",
    "rfr_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae78d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSE_rfr = np.sqrt(mean_squared_error(pred_B_y, rfr_pred))\n",
    "RMSE_rfr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb36f6a",
   "metadata": {},
   "source": [
    "#### Second a Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f6a380",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = LinearRegression().fit(X_train1, y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d53f293",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_pred = lm.predict(pred_B_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3902f209",
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSE_lm = np.sqrt(mean_squared_error(pred_B_y, lm_pred))\n",
    "RMSE_lm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f306e99a",
   "metadata": {},
   "source": [
    "## Considering the mean was 15.6 dollars.  Our RMSE look pretty off.  Let's check why."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e51b2a",
   "metadata": {},
   "source": [
    "## Looking at actual and predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b135741f",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_df = pd.DataFrame({'Actual Values':pred_B_y, 'RandForst Predicted Values':rfr_pred, 'LinRegress Predicted Values':lm_pred})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8415d246",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f63adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_df[compare_df['Actual Values']!=0.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b143e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "round(compare_df['RandForst Predicted Values'].sum(),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8132eaad",
   "metadata": {},
   "outputs": [],
   "source": [
    "round(compare_df['Actual Values'].sum(),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c45fcfe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612c04ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
